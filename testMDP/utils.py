import numpy as np
import torch
import torch.nn.functional as F
from torch.distributions.categorical import Categorical

def softmax(x):
    exp = np.exp(x)
    return exp / np.sum(exp, axis=0, keepdims=True)

def generate_sequence_grid_world(model, gw, batches, device):
    """Generate sequence of states generated by the model's policy"""

    model.eval()
    max_iterations = gw.states

    x = torch.tensor(gw.get_start_state(batches)).to(device)
    generated_actions = []
    tgt = [(torch.ones(batches, 1) * gw.SOS).to(device)]
    rewards = []
    acts = []

    for i in range(gw.states):
        tgt_in = torch.cat(tgt, dim=1).to(torch.int32)
        policy = F.softmax(model(x, tgt_in, -1), dim=-1)[:,-1,:]
        distrib = Categorical(policy)
        actions = distrib.sample()
        acts.append(actions)
        actions= actions.cpu().numpy()
        state_mdp = x.cpu().numpy()
        if len(tgt) > 1:
            state_mdp = tgt[-1].cpu().numpy()

        #print(state_mdp)

        x_next, r, _ = gw.batched_step(state_mdp, actions)
        x_next = torch.tensor(x_next).to(device).unsqueeze(1)

        rewards.append(torch.tensor(r).to(device))
        tgt.append(x_next)

    tgt = torch.cat(tgt, dim=1)
    rewards = torch.stack(rewards, dim=1)
    acts = torch.stack(acts, dim=1)
    return x, tgt, rewards, acts

def generate_sequence(model, mdp, batches):
    """This is intended for the basic MDP                         """
    """Generate sequence of states generated by the model's policy"""
    model.eval()
    x = torch.tensor(mdp.get_start_state(batches)).to(device)
    generated_actions =  None
    tgt = None
    rewards = torch.zeros(batches)
    for i in range(5):
        policy = model(x, tgt)
        actions = torch.argmax(policy, dim=-1).detach().cpu()[:, -1]
        if generated_actions is None:
            generated_actions = actions.unsqueeze(1)
        else:
            generated_actions = torch.concat((generated_actions, actions.unsqueeze(1)), dim=1)
        actions = actions.numpy()
        state_numpy = x.detach().cpu().numpy()

        if tgt is not None:
            tgt_numpy = tgt.detach().cpu().numpy()
            state_numpy = tgt_numpy[:, -1, :]
        else:
            state_numpy = state_numpy.reshape((state_numpy.shape[0], state_numpy.shape[2]))

        x_next, r, _ = mdp.batched_step(state_numpy, actions)

        rewards = rewards + r

        x_next_torch = torch.tensor(x_next).to(device).unsqueeze(1)

        if tgt is None:
            tgt = x_next_torch
        else:
            tgt = torch.concat((tgt, x_next_torch), dim=1)

        model.train()

    return x, tgt, generated_actions.to(device), rewards.to(device)

def trainPPO(model, device, mdp, epochs, sub_epochs, batch_size, eps, c_1, c_2, optim):
    model_old = copy.deepcopy(model)
    loss = clippedLossVector()
    losses = []
    rewards = []
    for e in range(epochs):
        src, gen, acts, reward = generate_sequence(model, mdp, batch_size)

        # Generate EOS mask
        states = torch.argmax(gen, dim=-1)
        eos = (states == 1) | (states == 2) | (states == 4)
        first_eos = (eos.cumsum(dim=1) == 1)
        eos_mask = torch.logical_xor(eos, first_eos)
        eos_mask = torch.logical_not(eos_mask)

        temp_model = copy.deepcopy(model)
        avg_loss = 0
        for s_e in range(sub_epochs):
            optim.zero_grad()
            l = loss(model, model_old, eps, c_1, c_2, src, gen, acts, reward, eos_mask, device)
            l.backward()
            optim.step()
            avg_loss+=l.item()

        avg_loss = avg_loss/sub_epochs
        losses.append(avg_loss)
        avg_reward = reward.sum()/batch_size
        rewards.append(avg_reward.item())
        print("EPOCH", e+1)
        print("Average Loss", avg_loss)
        print("Average Reward", avg_reward)
        print("="*10)

        model_old = temp_model
    torch.save(model.state_dict(), "temp_model3.pth")
    return losses, rewards

