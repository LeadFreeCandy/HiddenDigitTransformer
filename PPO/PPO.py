import torch
import torch.nn as nn
import torch.nn.functional as F
import math



class clippedLossVector(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, model, model_old, eps, c_1, c_2, src, generated, actions, rewards, eos_mask, device, include_src_state=False):
        """
        Parameters:
            model    : current model for evaluating the policy
            model_old: previous model for evaluating the policy
            eps      : clip range parameter clip(x, 1-eps, 1+eps)
            c_1      : weight of MSE loss to total loss
            src      : src sequence
            generated: sequence generated by current policy
            actions  : Sequence of actions taken to generate generated
            rewards  : sum of rewards total for the episode (Assumes one reward at end of seq)
            eos_mask : mask of time points after the sequence has ended (False for t > T)
            device   : cpu / gpu
        """

        model.eval()
        model_old.eval()

        N = generated.shape[0]
        S = generated.shape[1]

        src = src.to(torch.int32)
        rewards = rewards.to(torch.float32)
        generated = generated.to(torch.int32)
        actions = actions.to(torch.int32)
        eos_mask = eos_mask.to(torch.int32)

        policy, value = model(src, generated, value=True)
        policy_old    = model_old(src, generated)
        value = value[:, :-1]


        policy = F.softmax(policy, dim=-1)
        policy_old = F.softmax(policy_old, dim=-1)

        entropy = -(policy * torch.log(policy)).sum(dim=-1)
        entropy_loss = (1/torch.numel(entropy)) * entropy.sum()

        batch_index = torch.arange(N).unsqueeze(1).expand(N,S)
        state_index = torch.arange(S).expand(N,S)
        actions = actions.to(torch.int32)


        pi = policy[batch_index, state_index, actions]
        pi_old = policy_old[batch_index, state_index, actions]

        
        r = torch.exp(torch.log(pi) - torch.log(pi_old))
        r_clipped = torch.clamp(r, min=1-eps, max=1+eps)

        rewards = rewards.unsqueeze(1).expand(N,S)
        value = value.squeeze(-1)
        A = rewards - value


        l_clip = torch.min(r*A, r_clipped*A)  * eos_mask
        l_clip = l_clip.sum()/(N*eos_mask.sum())


        rewards = rewards * eos_mask
        value = value * eos_mask
        rewards = rewards.reshape(-1,1)
        value   = value.reshape(-1,1)


        l_value = F.mse_loss(value, rewards)

        model.train()
        model_old.train()

        return - l_clip - c_2 * entropy_loss + c_1 * l_value

class clippedLossSequential(nn.Module):

    def __init__(self, eps, c_1, c_2, pad_idx, device):
        super().__init__()
        self.eps = eps
        self.c_1 = c_1
        self.c_2 = c_2
        self.pad_idx = pad_idx
        self.device = device

    def get_gae(self, rewards, eos_mask):
        reward = rewards*eos_mask
        return torch.cumsum(reward.flip(dims=[1]), dim=1).flip(dims=[1])

    def forward(self, model, model_old, src, generated, actions, rewards, eos_mask):

        generated = generated[:, :-1]

        """
        print("SRC")
        print(src)
        print("GENERATED")
        print(generated)
        print("ACTIONS")
        print(actions)
        print("REWARDS")
        print(rewards)
        print("EOS")
        print(eos_mask)
        """
        


        model.eval()
        model_old.eval()

        N = generated.shape[0]
        S = generated.shape[1]

        policy, value = model(src, generated, self.pad_idx, value=True)
        policy_old    = model_old(src, generated, self.pad_idx)
        policy = F.softmax(policy, dim=-1)
        policy_old = F.softmax(policy_old, dim=-1)

        """
        print("VALUE")
        print(value)
        print("POLICY")
        print(policy)
        print("POLICY OLD")
        print(policy_old)
        """

        entropy = -(policy*torch.log(policy)).sum(dim=-1)
        entropy_loss = (1/torch.numel(entropy)) * entropy.sum()

        batch_index = torch.arange(N).unsqueeze(1).expand(N,S).to(self.device)
        state_index = torch.arange(S).expand(N,S).to(self.device)

        pi = policy[batch_index, state_index, actions]
        pi_old = policy_old[batch_index, state_index, actions]

        """
        print("PI")
        print(pi)
        print("PI_OLD")
        print(pi_old)
        """

        r = torch.exp(torch.log(pi) - torch.log(pi_old))
        r_clipped = torch.clamp(r, min=1-self.eps, max=1+self.eps)

        eos_mask_trimmed = eos_mask[:, :-1]

        gae = self.get_gae(rewards, eos_mask_trimmed).to(torch.float32)
        value = value.squeeze(-1)
        A = (gae - value)

        l_clip = (torch.min(r*A, r_clipped*A) * eos_mask_trimmed)
        l_clip = l_clip.sum()/(N*eos_mask_trimmed.sum())

        gae = gae * eos_mask_trimmed
        value = value * eos_mask_trimmed
        gae = gae.reshape(-1,1) 
        value   = value.reshape(-1,1)

        l_value = F.mse_loss(value, gae)

        model.train()
        model_old.train()


        
        return -l_clip + self.c_1*l_value - self.c_2 * entropy_loss



