import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class clippedLossVector(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, model, model_old, eps, c_1, c_2, src, generated, actions, rewards, eos_mask, device, include_src_state=False):
        """
        Parameters:
            model    : current model for evaluating the policy
            model_old: previous model for evaluating the policy
            eps      : clip range parameter clip(x, 1-eps, 1+eps)
            c_1      : weight of MSE loss to total loss
            src      : src sequence
            generated: sequence generated by current policy
            actions  : Sequence of actions taken to generate generated
            rewards  : sum of rewards total for the episode (Assumes one reward at end of seq)
            eos_mask : mask of time points after the sequence has ended (False for t > T)
            device   : cpu / gpu
        """

        model.eval()
        model_old.eval()

        N = generated.shape[0]
        S = generated.shape[1]

        src = src.to(torch.int32)
        rewards = rewards.to(torch.float32)
        generated = generated.to(torch.int32)
        actions = actions.to(torch.int32)
        eos_mask = eos_mask.to(torch.int32)


        #pad_mask = torch.logical_not(tgt == pad_idx).to(torch.int64).to(device)
        #eos_mask = torch.logical_not(eos_mask)

        policy, value = model(src, generated, value=True)
        policy_old    = model_old(src, generated)
        value = value[:, :-1]


        policy = F.softmax(policy, dim=-1)
        policy_old = F.softmax(policy_old, dim=-1)

        entropy = -(policy * torch.log(policy)).sum(dim=-1)
        entropy_loss = (1/torch.numel(entropy)) * entropy.sum()

        batch_index = torch.arange(N).unsqueeze(1).expand(N,S)
        state_index = torch.arange(S).expand(N,S)
        actions = actions.to(torch.int32)


        pi = policy[batch_index, state_index, actions]
        pi_old = policy_old[batch_index, state_index, actions]
        
        r = torch.exp(torch.log(pi) - torch.log(pi_old))
        r_clipped = torch.clamp(r, min=1-eps, max=1+eps)

        rewards = rewards.unsqueeze(1).expand(N,S)
        value = value.squeeze(-1)
        A = rewards - value


        l_clip = torch.min(r*A, r_clipped*A)  * eos_mask
        l_clip = l_clip.sum()/(N*eos_mask.sum())


        rewards = rewards * eos_mask
        value = value * eos_mask
        rewards = rewards.reshape(-1,1)
        value   = value.reshape(-1,1)


        l_value = F.mse_loss(value, rewards)

        model.train()
        model_old.train()

        return - l_clip - c_2 * entropy_loss + c_1 * l_value

class clippedLossSequential(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, model, model_old, eps, c_1, c_2, src, generated, rewards, pad_idx, eos_idx, device):

        model.eval()
        model_old.eval()

        N = generated.shape[0]
        S = generated.shape[1]

        eos_mask  = (generated==eos_idx)
        first_eos = ((generated == eos_idx).cumsum(dim=1).cumsum(dim=1) == 1)
        eos_mask = torch.logical_xor(eos_mask.cumsum(dim=1), first_eos)
        eos_mask = torch.logical_not(eos_mask)

        policy, value = model(src, generated, pad_idx, value=True)
        policy_old    = model_old(src, generated, pad_idx)
        policy = F.softmax(policy, dim=-1)
        policy_old = F.softmax(policy_old, dim=-1)

        entropy = -(policy*torch.log(policy)).sum(dim=-1)
        entropy_loss = (1/torch.numel(entropy)) * entropy.sum()

        batch_index = torch.arange(N).unsqueeze(1).expand(N,S)
        state_index = torch.arange(S).expand(N,S)
        pi = policy[batch_index, state_index, generated]
        pi_old = policy_old[batch_index, state_index, generated]
        
        r = torch.exp(torch.log(pi) - torch.log(pi_old))
        r_clipped = torch.clamp(r, min=1-eps, max=1+eps)


        rewards = rewards.unsqueeze(1).expand(N,S)
        value = value.squeeze(-1)
        A = (rewards - value)

        #l_clip = torch.min(r*A, r_clipped*A) 
        l_clip = (torch.min(r*A, r_clipped*A) * eos_mask)
        l_clip = l_clip.sum()/(N*eos_mask.sum())

        rewards = rewards * eos_mask
        value = value * eos_mask
        rewards = rewards.reshape(-1,1) 
        value   = value.reshape(-1,1)

        l_value = F.mse_loss(value, rewards)

        model.train()
        model_old.train()


        return -l_clip + c_1*l_value - c_2 * entropy_loss

